{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T03:31:18.043Z",
     "iopub.execute_input": "2025-01-02T03:31:13.170944Z",
     "iopub.status.busy": "2025-01-02T03:31:13.170524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from accelerate.utils import write_basic_config\n",
    "\n",
    "# write_basic_config()\n",
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/cu124/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "def extract_component(pickle_file, image_path_root, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    for k in tqdm(data.keys()):\n",
    "        components = data[k]['components']\n",
    "        for comp in components:\n",
    "            if os.path.exists(os.path.join(save_dir,f\"{comp['object_id']}.png\")):\n",
    "                continue\n",
    "            try:\n",
    "              img = Image.open(f\"{os.path.join(image_path_root,k)}_page-{comp['page']}.png\").convert(\"RGB\")\n",
    "              bbox = comp['bbox']\n",
    "              cropped_img = transforms.functional.crop(img,top=bbox[1],left=bbox[0],height=bbox[3],width=bbox[2])\n",
    "              cropped_img.save(os.path.join(save_dir,f\"{comp['object_id']}.png\"))\n",
    "            except Exception as e:\n",
    "              print(comp)\n",
    "              print(e)\n",
    "              print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_component(\"val_data.pkl\",\"val/val\",\"components\")\n",
    "extract_component(\"test_data.pkl\",\"test/test\",\"components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "from transformers import AutoImageProcessor\n",
    "import torch\n",
    "class CompVisualDataset(Dataset):\n",
    "    def __init__(self, pickle_file,image_path_root):\n",
    "        super().__init__()\n",
    "        with open(pickle_file, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        self.components = []\n",
    "        self.root_path = image_path_root\n",
    "        for k in data.keys():\n",
    "            for comp in data[k]['components']:\n",
    "                if comp['bbox'] == [0.0, 0.0, 0.0, 0.0]:\n",
    "                  continue\n",
    "                self.components.append(comp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.components)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comp = self.components[index]\n",
    "        img = Image.open(os.path.join(self.root_path, f\"{comp['object_id']}.png\")).convert(\"RGB\")\n",
    "        return img, comp['object_id']\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = [e[0] for e in batch]\n",
    "    object_ids = [e[1] for e in batch]\n",
    "    return imgs, object_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_val_dataset = CompVisualDataset('val_data.pkl','components')\n",
    "visual_test_dataset = CompVisualDataset('test_data.pkl','components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "class VisualEncoder(torch.nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.dinvov2 = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "        self.dinvov2.config.return_dict=False\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.dinvov2(pixel_values)\n",
    "        sequence_outputs = outputs[0]\n",
    "        cls_token = sequence_outputs[:,0]\n",
    "        patch_tokens = sequence_outputs[:,1:]\n",
    "        embedding = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1)\n",
    "        return embedding\n",
    "encoder = VisualEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisualEncoder()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using: {device}\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    traced_model = torch.jit.trace(model, torch.rand(1,3,224,224).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "def extract_features(dataloader, feature_path):\n",
    "  if not os.path.exists(feature_path):\n",
    "    os.makedirs(feature_path)\n",
    "  with torch.no_grad():\n",
    "      for imgs, object_ids in tqdm(dataloader):\n",
    "          image_inputs = image_processor(imgs, return_tensors=\"pt\").to(device)\n",
    "          features = model(image_inputs.pixel_values)\n",
    "          for idx, obj_id in enumerate(object_ids):\n",
    "            torch.save(features[idx],os.path.join(feature_path,f\"{obj_id}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_val_dataloader = DataLoader(visual_val_dataset,batch_size=32, collate_fn= collate_fn, num_workers=4)\n",
    "visual_test_dataloader = DataLoader(visual_test_dataset,batch_size=32, collate_fn= collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features(visual_val_dataloader,  'visual_features')\n",
    "print(\"Extraction completed for val set!\")\n",
    "extract_features(visual_test_dataloader,  'visual_features')\n",
    "print(\"Extraction completed for test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "graphs = []\n",
    "cat_no_rel = ['other',\n",
    " 'report_title',\n",
    " 'title',\n",
    " 'table_of_contents',\n",
    " 'cross',\n",
    " 'list_of_tables',\n",
    " 'appendix_list',\n",
    " 'references',\n",
    " 'list_of_figures']\n",
    "\n",
    "def generate_graphs(pkl_file, graphs):\n",
    "    with open(pkl_file,\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    for doc in tqdm(data.keys()):\n",
    "        components = data[doc]['components']\n",
    "        nodes = sorted([comp['object_id'] for comp in components if comp['category'] not in cat_no_rel])\n",
    "        edges = torch.combinations(torch.arange(len(nodes)), r=2) #NC2\n",
    "        g = dgl.DGLGraph()\n",
    "        g.add_nodes(len(nodes))\n",
    "        g.add_edges(edges[:,0],edges[:,1])\n",
    "        g.add_edges(edges[:,1],edges[:,0])\n",
    "        g.ndata['obj_id'] = torch.tensor(nodes)\n",
    "        graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "graphs = generate_graphs(\"val_data.pkl\",graphs)\n",
    "graphs = generate_graphs(\"test_data.pkl\",graphs)\n",
    "dgl.save_graphs(\"graphs.bin\",graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import dgl\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graph_file, feature_dir):\n",
    "        super().__init__()\n",
    "        self.graphs,_ = dgl.load_graphs(graph_file)\n",
    "        self.feature_dir = feature_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def load_feat(self,nodes):\n",
    "        tensors = []\n",
    "        for idx, node in enumerate(nodes):\n",
    "            try:\n",
    "                tensors.append(torch.load(f\"{self.feature_dir}/{node}.pt\",map_location=torch.device(\"cpu\"),weights_only=False).unsqueeze(0))\n",
    "            except:\n",
    "                tensors.append(torch.zeros((1,1536)))\n",
    "        return torch.cat(tensors,dim=0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        g = self.graphs[index]\n",
    "        nodes = g.ndata['obj_id']\n",
    "        feats = self.load_feat(nodes) if len(nodes) != 0 else None\n",
    "        return g, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "import torch.nn.functional as F\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, 1)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        \"\"\"\n",
    "        Computes a scalar score for each edge of the given graph.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        edges :\n",
    "            Has three members ``src``, ``dst`` and ``data``, each of\n",
    "            which is a dictionary representing the features of the\n",
    "            source nodes, the destination nodes, and the edges\n",
    "            themselves.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary of new edge features.\n",
    "        \"\"\"\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPPredictor(1536)\n",
    "model.load_state_dict(torch.load(\"predictor.pth\",weights_only=True))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphDataset(\"graphs.bin\",\"visual_features\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=lambda batch: batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_edges= []\n",
    "for g, feats in tqdm(dataloader):\n",
    "    if feats:\n",
    "        scores = model(g.to(device),feats.to(device))\n",
    "        edges = g.edges()\n",
    "        nodes = list(g.ndata['obj_ids'])\n",
    "        p_eids = torch.where(scores >= threshold)\n",
    "        p_u_edge = list(edges[0][p_eids])\n",
    "        p_v_edge = list(edges[1][p_eids])\n",
    "        get_obj_id = lambda x: nodes.index(x)\n",
    "        predicted_edges = list(zip(list(map(get_obj_id,p_u_edge)),list(map(get_obj_id,p_v_edge))))\n",
    "        all_predicted_edges.append(predicted_edges)\n",
    "    else:\n",
    "        all_predicted_edges.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['ID','Parent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_relations =[('summary', 'paragraph'),\n",
    " ('figure', 'figure_caption'),\n",
    " ('table', 'table_caption'),\n",
    " ('form_title', 'form_body'),\n",
    " ('section', 'subsection'),\n",
    " ('subsection', 'subsubsection'),\n",
    " ('section', 'paragraph'),\n",
    " ('subsubsection', 'paragraph'),\n",
    " ('paragraph', 'list'),\n",
    " ('subsubsection', 'subsubsubsection'),\n",
    " ('subsubsubsection', 'paragraph'),\n",
    " ('subsection', 'paragraph'),\n",
    " ('subsection', 'list'),\n",
    " ('summary', 'form_body'),\n",
    " ('summary', 'form'),\n",
    " ('abstract', 'form'),\n",
    " ('abstract', 'form_body'),\n",
    " ('subsection', 'form_body'),\n",
    " ('subsubsection', 'form_body'),\n",
    " ('section', 'list'),\n",
    " ('section', 'form_body'),\n",
    " ('abstract', 'paragraph'),\n",
    " ('section', 'form'),\n",
    " ('subsubsection', 'list'),\n",
    " ('subsubsubsection', 'list'),\n",
    " ('subsection', 'form'),\n",
    " ('subsubsubsection', 'subsubsubsubsection'),\n",
    " ('subsubsubsubsection', 'paragraph'),\n",
    " ('subsubsubsection', 'form_body'),\n",
    " ('subsubsection', 'form'),\n",
    " ('subsubsubsection', 'form')]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10197347,
     "sourceId": 86960,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 206783,
     "modelInstanceId": 184626,
     "sourceId": 216553,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "vrd-iu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
